---
title: "Veckoschema"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
```

## **v23**: Introduktion till kursverktyg

Den första veckan kommer innehålla administration och avslutas med en provkörning av kursverktygen i laboration 1.

### Mål denna vecka
- Dina kurskonto på [GitHub](https://github.com/)  och [RStudio Cloud](https://rstudio.cloud/) skall vara kopplade till kursens arbetsytor.
- Första laborationen inlämnad. (Deadline söndag).

### Kursmaterial

- Hadley Wickham and Garrett Grolemund, *R for Data Science* (i fortsättningen R4DS): [Kapitel 1, Introduction](https://r4ds.had.co.nz/introduction.html).
- [Kursintroduktion (YouTube, 14:34)](https://youtu.be/gQUjg5cMUGU)
- [RStudio Cloud introduktion(YouTube, 12:52)](https://youtu.be/hsJTGkrsQkc)
- [RMarkdown introduktion (YouTube, 14:42)](https://youtu.be/Ckd3Yfkx5Hk)
- [GitHub introduktion (YouTube, 13:05)](https://youtu.be/nmxg_wthhHQ)
- [Examination och betyg (kurshemsida)](https://kurser.math.su.se/pluginfile.php/109552/mod_resource/content/1/Betygskriterier.pdf)

***

## **v24**: Enkla visualiseringar

### Mål denna vecka

- Att ha grundläggande förståelse för Rs datatyper.
- Att kunna skapa enkla diagram och förstå de grundläggande komponenterna i en `ggplot`-visualisering (`data`, `mapping`, `geom`).


### Kursmaterial

- R4DS: [Kapitel 2-4](https://r4ds.had.co.nz/introduction.html).

RStudio Primers: 

- [The Basics](https://rstudio.cloud/learn/primers/1)
- [Working with tibbles](https://rstudio.cloud/learn/primers/2.1)

### Övningar

Under rubriken övningar hittar du träningsmaterial som inte ingår i kursens examination. Detta ger bra övning inför inlämningsuppgifterna, med fördelen att du kan fråga och tipsa hur mycket du vill på kursens Discord. Skapa ett separat projekt i RStudio Cloud där du arbetar med övningar och skapa en mapp `data` i detta projekt där du kan spara de datamaterial vi använder.


Gör inte bara uppgifterna utan experimentera med data och försök hitta på egna frågeställningar att besvara, fråga på Discord om något inte blev som du tänkt. 

#### Ettor på stan

Du kan ladda ner en kopia av data från inlämningsuppgift 1 med
```{r, eval = FALSE, echo = TRUE}
download.file("https://github.com/MT3003-ST21/data/raw/main/booli_ettor_2021-05-28.csv",
              "data/booli_ettor_2021-05-28.csv")
```
och läsa in den med
```{r, eval = FALSE, echo = TRUE}
booli_ettor <- read_csv("data/booli_ettor_2021-05-28.csv")
```
Notera att det går bra att läsa direkt från en url, som i
```{r, eval = FALSE, echo = TRUE}
booli_ettor <- read_csv("https://github.com/MT3003-ST21/data/raw/main/booli_ettor_2021-05-28.csv")
```
men vi föredrar att ladda ner filen en gång, då kan vi vara säkra på att vi arbetar med samma material nästa gång vi öppnar projektet. Använd din egen fantasi för att undersöka materialet med enkla figurer, till exempel kan du undersöka

- Hur påverkar yta (`livingArea`) hyran (`rent`)?
- SVT rapporterade i våras om [rekordmånga anmälningar om lockpriser](https://www.svt.se/nyheter/inrikes/rekordmanga-anmalningar-om-lockpriser). Undersök hur förekomsten av lockpriser varierat genom att plotta  kvoten mellan slutpris och utgångspris (`soldPrice / listPrice`) mot försäljningsdatum (`soldDate`). Tips: Om figuren förstörs av ett extremt värde kan du zooma in genom att lägga till  `+ ylim(c(0, 2))` för att bestämma gränserna på `y`-axeln.
- Vilka veckodagar publicerar mäklarna sina annonser? Gör ett stapeldiagram över antalet annonser per veckodag, veckodag får du genom `weekdays(published)`.

*Faktorvariabler:* Ovanstående övning illustrerar ett vanligt problem. Veckodagarna blir sorterade i alfabetisk ordning medan vi antagligen vill ha dem i kronologisk. Problemet kommer av att en variabel av typen `character` inte innehåller någon information om kategoriernas ordning, det gör däremot en variabel av typen `factor`. För att ändra ordning behöver vi alltså göra om `weekdays(published)` till en faktorvariabel. Om vi istället för `weekdays(published)` använder `factor(weekdays(published), levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))` så ordnas staplarna i den ordning vi angett. 


#### Systembolagets katalog

Systembolaget brukade ha hela sin katalog öppen för nedladdning, [så är inte längre fallet](https://api-portal.systembolaget.se/api-update-blog/changes-in-the-api-portal) men en kopia från 2019 finns sparad i `systembolaget_2019-10-30.csv`. Ladda ner den med

```{r, eval = FALSE, echo = TRUE}
download.file("https://github.com/MT3003-ST21/data/raw/main/systembolaget_2019-10-30.csv",
              "data/systembolaget_2019-10-30.csv")
```

och läs in med

```{r, eval = FALSE, echo = TRUE}
bolaget <- read_csv("data/systembolaget_2019-10-30.csv")
```

skapa dig en överblick genom att använda t.ex. `glimpse` eller `View`. Ett enkelt sätt att få en överblick över en viss kategorisk variabel (om kategorierna inte är för många) är ett stapeldiagram med `geom_bar`, prova till exempel med variablerna `Varugrupp` eller `Ursprungsland`. Man kan koppla den kategoriska variabeln antingen till y eller x-axeln (med `aes(x = Varugrupp)` respektive `aes(y = Varugrupp)`). Vilket passar bäst?

Vilken varugrupp har högst literpris? Gör ett punktdiagram med `Varugrupp` på `y`-axeln och `PrisPerLiter` på `x`-axeln. Eftersom fördelningen för `PrisPerLiter` kan det öka läsbarheten att använda en logaritmisk skala på `x`-axeln genom att lägga till `+ scale_x_log10()` till anropet. Som med veckodagarna ovan är det inte optimalt att ordna kategorierna alfabetiskt, prova att använda `y = fct_reorder(Varugrupp, PrisPerLiter)` istället för `y = Varugrupp`. Funktionen `fct_reorder` gör om `Varugrupp` till faktorvariabel och ordnar kategorierna efter `PrisPerLiter` (medianen i varje grupp).

Med hjälp av `filter` kan vi begränsa oss till vissa kategorier (mer om detta nästa vecka). Vad genererar till exempel följande kod?
```{r, eval = FALSE, echo = TRUE}
bolaget_subdata <- filter(bolaget, 
                      Ursprunglandnamn %in% c("Sverige", "Storbritannien", "Frankrike", "Spanien"), 
                      Varugrupp %in% c("Öl", "Rött vin", "Whisky", "Aperitif och dessert"))
ggplot(data = bolaget_subdata,
       mapping = aes(y = Ursprunglandnamn)) + 
  geom_bar() + 
  facet_wrap(~Varugrupp, scales = "free_x")
```


#### Gapminder data

[Gapminder foundation](https://www.gapminder.org/) verkar för att förändra människors syn på världen med data. Titta gärna på något av [Hans Roslings föredrag](https://www.ted.com/speakers/hans_rosling?language=sv) eller använd deras [interaktiva verktyg](https://www.gapminder.org/tools/#$chart-type=bubbles&url=v1) och försök återskapa figurerna med `ggplot`. En begränsad del av deras data har sammanställts i `gapminder_2021-06-14.csv` (se [dokumentation](https://www.gapminder.org/data/documentation/)). Ladda ner med


```{r, eval = FALSE, echo = TRUE}
download.file("https://github.com/MT3003-ST21/data/raw/main/gapminder_2021-06-14.csv",
              "data/gapminder_2021-06-14.csv")
```
och läs in med
```{r, eval = FALSE, echo = TRUE}
gapminder <- read_csv("data/gapminder_2021-06-14.csv")
```



***

## **v25**: Sammanfatta en tabell


### Mål denna vecka

- Att kunna sortera och filtrera med `arrange` och `filter`.
- Skapa nya kolumner med `mutate`
- Sammanställningar med `group_by` och `summarise`

### Kursmaterial

- R4DS: [Kapitel 5-6](https://r4ds.had.co.nz/transform.html).

RStudio Primers:

- [Isolating data with dplyr](https://rstudio.cloud/learn/primers/2.2)
- [Deriving information with dplyr](https://rstudio.cloud/learn/primers/2.3)

### Övningar

#### Ettor på stan

- Använd `arrange` (eventuellt i kombination med `select` för att begränsa utskriften) för att verifiera att bostaden med äldst `constructionYear` ligger i vad som troligen är [Stockholms äldsta bevarande bostadshus](https://sv.wikipedia.org/wiki/Baggensgatan_27). För flera rader antar `constructionYear` värdet `NA`, värde saknas. Hur hanteras detta av `arrange`?

- Använd `count` och `arrange` för att hitta den gatuadress där det sålts flest ettor. Detta är [Kvinnornas hus](https://stockholmskallan.stockholm.se/post/32632) som huvudsakligen består av ettor med kokvrå.

- Inför variabeln `priceRise` som prisökningen i procent (baserat på `soldPrice` och `listPrice`) och bestäm medelvärdet av `priceRise` per mäklare (`source.name`) för alla mäklare med minst 100 försäljningar och sammanfatta med ett histogram. **OBS**: Hur hanterar funktionen `mean` saknade värden? En försäljning i materialet sticker ut, kan du hitta och filtrera bort den?

#### Gapminder data

- Gör en tabell över medelinkomsten per capita för varje kontinent 2020. Tänk på att även `income` är räknat i per capita, du behöver alltså bestämma kontinenternas totala inkomst och dela på den totala populationen.

#### Systembolagets katalog

- Illustrera hur medelpriset per liter på Röda viner beror på årgång (du kan begränsa dig till årgångar med minst 10 produkter).
- Illustrera hur medelpriset per liter på Röda viner beror på ursprungsland med ett stapeldiagram, ordna staplarna efter medelpris (du kan begränsa dig till länder med minst 10 produkter).
- Kolumnen `Alkoholhalt` är kodad som textvariabel eftersom den innehåller ett procenttecken, gör om den till numerisk genom att först ta bort procenttecknet med `str_remove` och sedan konvertera med `as.numeric`. Illustrera sedan hur pris per liter beror på alkoholhalt för varugruppen öl i ett punktdiagram med en rät trendlinje (`geom_smooth(method = "lm")`). 
- [Skattesatsen för öl](https://skatteverket.se/foretag/skatterochavdrag/punktskatter/alkoholskatt/skattesatser.4.4a47257e143e26725aecb5.html) är 2.02 kronor per liter och volymprocent, bestäm en ny variabel `SkattPerLiter` som innehåller skattesatsen. Plotta sedan `PrisPerLiter - SkattPerLiter` mot `Alkoholhalt` (för varugruppen öl) med trendlinje.


#### Badväder

Hos Havs och Vattenmyndigheten kan du hitta [statistik över vattenprover](https://havbi.havochvatten.se/analytics/saw.dll?PortalPages) tagna vid badplatser runt om i landet, ett utdrag i Excel-format kan laddas ned med 
```{r, eval = FALSE, echo = TRUE}
download.file("https://github.com/MT3003-ST21/data/raw/main/Provresultat.xlsx",
              "data/Provresultat.xlsx")
```
och läsas in med
```{r, eval = FALSE, echo = TRUE}
badvatten <- readxl::read_excel("data/Provresultat.xlsx") %>% 
  fill(Län, Kommun)
```

Vad gör funktionen `fill` i detta fall? Undersök materialet närmare avseende t.ex. skillnader mellan län, EU-bad/Övriga och Badplatstyp. En svårighet är att man för E.coli och enterokocker ibland använder ett mätvärde och ibland en övre gräns (se prefix-kolumnerna). 

***

## **v26**: Utforska data med visualiseringar

### Mål denna vecka

- Skriva en enkel rapport i Rmarkdown.
- Utforska data med `ggplot`.
- Andra laborationen inlämnad (deadline söndag).

### Kursmaterial

- R4DS: [Kapitel 7](https://r4ds.had.co.nz/exploratory-data-analysis.html), [Kapitel 27](https://r4ds.had.co.nz/r-markdown.html) och [Kapitel 28](https://r4ds.had.co.nz/graphics-for-communication.html). 


RStudio Primers:

- [Visualize Data](https://rstudio.cloud/learn/primers/3)

### Övningar

Inga speciella övningar denna vecka, experimetera med visualiseringar av data från tidigare veckor.


## **v27**: Läsa data från fil

### Mål denna vecka

- Att kunna importera tabeller från textfiler.

### Kursmaterial

- R4DS: [Kapitel 9-11](https://r4ds.had.co.nz/data-import.html).

#### Kapitel 11 från ett svenskt perspektiv

Paketet `readr` följer amerikansk standard om du inte ber det göra något annat, här listar vi några skillnader mellan svensk och amerikansk standard som kan vara bra att ha koll på.

#####  Filformatet `.csv`

`csv` står för *comma separated values* och är ett av de vanligaste formaten för att spara tabeller i en textfil. I en `csv`-fil motsvarar varje rad en rad i tabellen (översta raden består ofta av kolumnrubriker) och kommatecken används för att separera kolumner. Filen med innehåll 
```
x, y
1, 2
3, 4
```
motsvarar alltså tabellen 
```{r, echo = TRUE}
readr::read_csv(
"x, y
1, 2
3, 4")
``` 

Detta fungerar bra i engelskspråkiga länder där man använder punkt som decimaltecken. När man som i svensk standard använder kommatecken som decimaltecken används istället ofta semikolon som kolumnavgränsare. Så är till exempel fallet i Excel; om du har Excel med svenska nationella inställningar och väljer att spara en tabell i formatet `CSV (kommaavgränsad) *.csv` så sparas den i själva verket som en fil där kolumner avgränsas med semikolon. Försöker du sedan läsa in den med `read_csv` placeras alla värden i samma kolumn 
```{r, echo = TRUE}
readr::read_csv(
"x; y
1; 2
3; 4")
```
Eftersom detta är vanligt förekommande finns en specialfunktion `read_csv2` som utgår ifrån att kolumner skiljs med semikolon och komma används som decimaltecken
```{r, echo = TRUE}
readr::read_csv2(
"x; y
1; 2
3; 4")
```

##### Decimaltecken igen

När siffror kombineras med punkt, kommatecken och mellanslag gissar paketet `readr` vad dessa betyder utifrån amerikansk standard. Här kan det lätt bli fel om vi slarvar med att ange lokala inställningar (`locale`). Jämför till exempel
```{r, echo = TRUE}
readr::parse_number(c("3,14", "3 000"))
```
med
```{r, echo = TRUE}
readr::parse_number(c("3,14", "3 000"), locale = readr::locale(decimal_mark = ",", 
                                                               grouping_mark = " "))
```

Ibland får man lösa konverteringen själv. I data som matats in manuellt är det inte ovanligt att både punkt och kommatecken förekommer som decimaltecken om vartannat i samma fil. Då är det säkrare att läsa in alla kolumner som text (`chr`) och använda till exempel `str_replace(x, ",", ".")` följt av `as.numeric`.

##### Spagetti med kÃ¶ttfÃ¤rssÃ¥s

Antagligen har du någon gång stött på en text där svenska tecken bytts ut mot obegripliga symboler. Problemet beror på att det utvecklats många olika konventioner för [teckenkodning](https://sv.wikipedia.org/wiki/Teckenkod) som kan vara specifika för operativsystem och applikationer. De viktigaste att känna till är [UTF-8](https://sv.wikipedia.org/wiki/UTF-8), som är standard på webben och kan koda alla möjliga språk, och [ISO 8859-1](https://sv.wikipedia.org/wiki/ISO/IEC_8859-1) eller **latin1**, som är en enklare teckenkodning anpassad för västeropeiska språk. Paketet `readr` förutsätter alltid UTF-8, medan motsvarande funktioner i Rs `base`-paket (t.ex. `read.csv` som motsvarar `readr::read_csv`) anpassar sig efter lokala systeminställningar.

Ser ditt resutat ut som i rubriken har du antagligen försökt läsa in "köttfärssås" som latin1 när den i själva verket var kodad i UTF-8, detta är mindre vanligt med just `readr`-paketet eftersom det läser in som UTF-8 om du inte anger något annat. Om du däremot försöker läsa in en fil kodad i latin1 med `readr` blir det  `k\xf6ttf\xe4rss\xe5s` om du inte anger `locale = locale(encoding = "latin1")`.

### Övningar

#### SCB-data

I [SCBs statistikdatabas](https://www.statistikdatabasen.scb.se/pxweb/sv/ssd/) kan man skapa en länk (url) till tabeller man tagit fram. Filen på `https://www.statistikdatabasen.scb.se/sq/110431` innehåller medelålder per län i formatet *Kommaavgränsad med rubrik*. Undersök den med `read_lines("https://www.statistikdatabasen.scb.se/sq/110431")` och försök sedan läsa in den med `read_csv`.

#### Mer badvatten

Badvattenfilen som användes i tidigare övningar var exporterad som Excel-fil från [Hav och Vattenmyndigheten](https://havbi.havochvatten.se/analytics/saw.dll?PortalPages), om vi istället väljer att exportera som csv får vi en fil som kan hämtas med
```{r, eval = FALSE, echo = TRUE}
download.file("https://github.com/MT3003-ST21/data/raw/main/Provresultat_2021-06-28.csv",
              "data/Provresultat_2021-06-28.csv")
```

Försök läsa in den med `read_csv`, vad händer med vattentemperaturen? Fixa!

***

## **v28**: Omforma och sammanfläta tabeller


### Mål denna vecka

- Att kunna omforma tabeller till ett "tidy"-format.
- Att kunna skapa nya tabeller genom att sammanfoga flera tabeller med gemensamma kolumner.

### Kursmaterial

- R4DS: [Kapitel 12-13](https://r4ds.had.co.nz/tidy-data.html)

RStudio primers:

- [Tidy your data](https://rstudio.cloud/learn/primers/4). **OBS**: I delen *Reshape Data* används funktionerna `gather` och `spread`, dessa har ersatts med `pivot_longer` och `pivot_wider` som är enklare att använda (de gamla fungerar dock fortfarande). Försök lösa övningen med `pivot_longer` och `pivot_wider`.

### Övningar

#### Gapminder data

På [gapminders dataarkiv](https://www.gapminder.org/data/) kan man ladda ner deras sammanställda datamaterial, en fil för varje variabel. Tabellen som använts i tidigare övningar är sammanställd av sådana filer. Filen med antal mobiltelefoner per land och år har vi hämtat så att den kan laddas ned med
```{r, eval = FALSE, echo = TRUE}
download.file("https://github.com/MT3003-ST21/data/raw/main/cell_phones_total_2021-06-27.csv",
              "data/cell_phones_total_2021-06-27.csv")
```
och sedan läsas in med 
```{r, eval = FALSE, echo = TRUE}
cell_phones_total <- read_csv("data/cell_phones_total_2021-06-27.csv",
                              col_types = cols(.default = "c"))
```
här ser `col_types = cols(.default = "c" )` till att alla kolumner läses in som text (`chr`) vilket kommer visa sig vara praktiskt. Du kan också prova med någon anna variabel du hämtad ned direkt från gapminder.

Uppgiften är nu att lägga till variabeln till gapminder-tabellen från tidigare övningar genom att

- Gör om `cell_phones_total` till "tidy"-format med `pivot_longer`.
- Lägg till den nya variabeln som en kolumn i gapminder tabellen med lämplig `*_join` (om du gör allt i en pipe-sekvens kan `right_join` vara lämpligt).
- Anledningen till att vi läste in alla kolumner som text är att gapminder valt att koda numeriska värden i olika enheter. Talet 1100 kodas t.ex. som `1.1k` och 12 600 000 som `12.6M`. Detta kan t.ex. lösas med `case_when` som nedan
```{r, eval = FALSE, echo = TRUE}
case_when(
  str_detect(antal_mobiler, "k") ~ 1000 * (str_remove(antal_mobiler, "k") %>% as.numeric()),
  str_detect(antal_mobiler, "M") ~ ...
```

#### Jämställdhet

Tabellen

```{r, echo = TRUE}
read_csv("https://www.statistikdatabasen.scb.se/sq/110673", 
         skip = 1, locale = locale(encoding = "latin1"))
```

innehåller antal svenska EU-parlamentariker efter kön och år. Använd `pivot_longer`, `pivot_wider`, `mutate` och `select` i en följd för att generera

```{r, echo = FALSE}
read_csv("https://www.statistikdatabasen.scb.se/sq/110673", 
         skip = 1, locale = locale(encoding = "latin1")) %>% 
  pivot_longer(-kön, names_to = "year") %>% 
  pivot_wider(-kön, names_from = "kön") %>% 
  mutate(percent_female = kvinnor / (män + kvinnor) * 100) %>% 
  select(year, percent_female)
```


#### Koroplet

Koropletkartor brukar användas när man vill visa hur en variabel varierar geografiskt, ofta med en indelning i områden som t.ex. län eller kommuner. Du kan ladda ner och packa upp en karta över sveriges län i som en så kallad shapefil ([hämtad från SCB](https://www.scb.se/hitta-statistik/regional-statistik-och-kartor/regionala-indelningar/digitala-granser/)) med
```{r, eval = FALSE, echo = TRUE}
download.file("https://github.com/MT3003-ST21/data/raw/main/LanSweref99TM.zip",
              "LanSweref99TM.zip")
unzip("LanSweref99TM.zip", exdir = "LanSweref99TM")
```
Paketet `sf` innehåller funktioner för att arbeta med geografiska data och kan bland annat läsa in en shapefil i tabellformat med `st_read` som sedan kan plottas med `geom_sf`
```{r, eval = FALSE, echo = TRUE}
library(sf)
karta_lan <- st_read("LanSweref99TM/Lan_Sweref99TM_region.shp", quiet = TRUE)
ggplot(karta_lan, aes(fill = LnNamn)) + geom_sf()
```

```{r, echo = FALSE, out.width = "50%"}
knitr::include_graphics("https://raw.githubusercontent.com/MT3003-ST21/mt3003-st21.github.io/main/img/lan.png")
```


Objektet `karta_lan` kan användas som en vanlig tabell/`tibble`. Det betyder att vi kan lägga till länsvisa variabler med `*_join` om vi vill färglägga efter något mer intressant än `LnNamn`. Prova tabellen med medelålder från tidigare övningar, en nyckel får du t.ex. genom att skapa en variabel `LnKod = str_sub(region, 1, 2)` i tabellen över medelålder.

Ett alternativ till `ggplot` för just kartframställning är paketet [`tmap`](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html) som kan vara enklare att använda men mindre flexibelt.


***

## **v29**: Arbeta med text, faktorer och datum.

### Mål denna vecka

- Att kunna använda verktyg i paketen `stringr`, `forcats` och `lubridate`.

### Kursmaterial


- R4DS: [Kapitel 14-16](https://r4ds.had.co.nz/strings.html) (14.3 om reguljära uttryck kan läsas kursivt, ni förväntas ha en grundläggande förståelse för konceptet men inte detaljerna).

### Övningar

Kapitel 14-16 innehåller specifika verktyg för att arbeta med text, faktorer och datum. Vi har redan stött på flera av dessa, t.ex. förekom `stringr::str_detect`, `forcats::fct_relevel` och `lubridate::yday` i Laboration 2. Funktionerna i `stringr` och `forcats` börjar alltid med `str_` respective `fct_` vilket gör det enkelt att leta efter dem i RStudios Console-fönster då man får upp en lista genom att inleda raden med dessa tecken (skriv `lubridate::` för motsvarande lista för detta paket). Specifika övningar ges i kursboken, här ger vi ett exempel på hur man kan arbeta med textdata.

#### En textanalys

På [Projekt Gutenberg](https://www.gutenberg.org/) finns en stor samling gratis e-böcker. Vi kan ladda ner Strindbergs Hemsöborna med

```{r, echo = TRUE}
download.file("https://www.gutenberg.org/cache/epub/30078/pg30078.txt",
              "data/hemsoborna.txt")
```
läsa ett valt stycke med

```{r, echo = TRUE}
read_lines("data/hemsoborna.txt", skip = 194, n_max = 21) %>% 
  str_c(collapse = " ")
```

och spara hela boken i en tibble med

```{r, echo = TRUE}
hemsoborna <- tibble(row = read_lines("data/hemsoborna.txt", 
                                       skip = 90, n_max = 10178)) %>% 
  filter(row != "") %>% 
  mutate(row_no = 1:n())
hemsoborna
```

här har vi klippt bort text som inte hör till boken med `skip` och `n_max`, tagit bort tomma rader och lagt till radnummer. För att analysera texten vidare delar vi upp den ord för ord, konverterar bokstäver till gemener och lägger in en räknare för kapitel

```{r, echo = TRUE}
hemsoborna_words <- hemsoborna %>% 
  mutate(word = str_extract_all(row, boundary("word"))) %>% 
  group_by(row_no) %>% 
  summarise(word = unlist(word)) %>% 
  ungroup() %>% 
  mutate(word = str_to_lower(word),
         chapter = cumsum(str_detect(word, "kapitlet")))
```

- Vad gör egentligen `chapter = cumsum(str_detect(word, "kapitlet"))`?

Vi kan nu få de vanligaste orden med 
```{r, echo = TRUE}
hemsoborna_words %>% count(word, sort = TRUE)
```
de flesta av dessa är så kallade stoppord, ord som är viktiga för att binda ihop texten men annars betydelsefattiga. För att ta bort dem hämtar vi en lista på svenska stoppord
```{r, echo = TRUE}
stopwords <- read_table("https://raw.githubusercontent.com/stopwords-iso/stopwords-sv/master/stopwords-sv.txt",
           col_names = "word")
```

- Använd `anti_join` för att ta bort alla stoppord från listan på de vanligaste orden.
- Illustrera resultatet med funktionen `wordcloud2::wordcloud2`.

Stämningsanalys (semtiment analysis) är en populär metod för att beskriva känslostämningar i texter (se till exempel [Jane Austen](https://juliasilge.com/blog/if-i-loved-nlp-less/) och [Donald Trump](http://varianceexplained.org/r/trump-tweets/)). I allmänhet bygger det på att man anväder ett lexikon som kvantifierar stämningen i enskilda ord, till exempel associerar ett posotivt numeriskt värde till "kärlek" och ett negativt till "svartsjuka". Ett svenskt sådant lexikon kan laddas ned från [språkbanken](https://spraakbanken.gu.se/) som
```{r, echo = TRUE}
sentiment_lex <- read_csv("https://svn.spraakdata.gu.se/sb-arkiv/pub/lmf/sentimentlex/sentimentlex.csv")
```

- Koppla ihop `hemsoborna_words` och `sentiment_lex` med en `inner_join` och illustrera hur det går utför för Carlsson genom att bestämma medelvärdet av känslostämningen (variabeln `strength`) för varje kapitel.


***

## **v30**: Funktioner och iteration

### Mål denna vecka

- Att kunna skriva enkla funktioner för att göra koden mer lättläst och enklare att underhålla.
- Att kunna arbeta med Rs list-format.
- Att kunna förenkla upprepade uppgifter med iteration.

### Kursmaterial


- R4DS: [Kapitel 17-21](https://r4ds.had.co.nz/program-intro.html) 

### Övningar

#### Booli figurer

Koden bakom en figur skapad med `ggplot` kan bli många rader om man vill finjustera detaljer. Skall man upprepa en figur för olika parametervärden är det därför praktiskt att skriva en funktion. Använd tabellen över alla försäljningar i innerstaden som laddas ned med
```{r, eval = FALSE, echo = TRUE}
download.file("https://github.com/MT3003-ST21/data/raw/main/booli_innerstad_2021-07-02.csv",
              "data/booli_innerstad_2021-07-02.csv")
booli_data <- read_csv("data/booli_innerstad_2021-07-02.csv")
```


- Konstruera en funktion 
```{r, eval = FALSE, echo = TRUE}
plot_price_ts <- function(data, title = "Pris per kvadratmeter i Stockholms innerstad"){
  ...
}
```
som skapar en figur motsvarande den i Laboration 1 baserat på givet `data`.

- Vill vi nu plotta t.ex. tvåor kan vi använda funktionen med anropet `booli_data %>% filter(rooms == 2) %>% plot_price_ts()`. Lägg till ett argument `rooms` till funktionen så att `plot_price_ts(booli_data, rooms = 2)` ger samma resultat.

#### Olympiska medaljer

På [https://sv.wikipedia.org/wiki/Medaljf%C3%B6rdelning_vid_olympiska_sommarspelen_2020](https://sv.wikipedia.org/wiki/Medaljf%C3%B6rdelning_vid_olympiska_sommarspelen_2020) hittar du aktuell medaljfördelning för OS i Tokyo. Tabellen kan hämtas med
```{r, echo = TRUE}
library(rvest)
url <- "https://sv.wikipedia.org/wiki/Medaljf%C3%B6rdelning_vid_olympiska_sommarspelen_2020"
read_html(url) %>% 
  html_element(".wikitable") %>% 
  html_table()
```
En motsvarande tabell för t.ex. OS i Rio kan hämtas genom att byta `2020` mot `2016`.

Konstruera en funktion
```{r, echo = TRUE}
get_medals <- function(year){
  ...
}
```
som returnerar medaljtabellen för OS-år `year` som nedan (glöm inte ta bort Total-raden)
```{r, echo = FALSE}
get_medals <- function(year){
  url <- str_c("https://sv.wikipedia.org/wiki/Medaljf%C3%B6rdelning_vid_olympiska_sommarspelen_", year)
  table <- read_html(url) %>% 
    html_element(".wikitable") %>% 
    html_table() %>% 
    filter(Land != "Total")
  table %>% 
    select(country = Land, Guld, Silver, Brons) %>% 
    mutate(year = year, .before = 1) %>% 
    pivot_longer(Guld:Brons, names_to = "class", values_to = "number") 
}
```
```{r, echo = TRUE}
get_medals(2016)
```

- Använd `map_df` med `seq(1948, 2020, by = 4)` och `get_medals` som argument för att skapa en tabell över medaljfördelningen för alla sommar OS sedan 1948.

#### Gapminder data

I en [tidigare övning](https://mt3003-st21.github.io/schedule.html#v28:_Omforma_och_sammanfl%C3%A4ta_tabeller) omformade vi en csv-fil från gapminder till ett mer användbart format. Nedanstående kod hämtar fler filer och placerar dem i `data/gapminder_raw`

```{r, eval=FALSE, echo = TRUE}
dir.create("data/gapminder_raw")
filenames <- c("cell_phones_total.csv", "child_mortality_0_5_year_olds_dying_per_1000_born.csv", 
               "children_per_woman_total_fertility.csv", "co2_emissions_tonnes_per_person.csv", 
               "income_per_person_gdppercapita_ppp_inflation_adjusted.csv", 
               "life_expectancy_years.csv", "population_total.csv")

urls <- str_c("https://github.com/MT3003-ST21/data/raw/main/gapminder_raw/", filenames)
destfiles <- str_c("gapminder_raw/", filenames)
walk2(urls, destfiles, download.file)
```

- Konstruera en funktion
```{r, eval = FALSE, echo = TRUE}
read_gapminder <- function(file, path = "data/gapminder_raw/"){
  ...
}
```
som genererar följande
```{r, echo = FALSE}
read_gapminder <- function(file, path = "data/gapminder_raw/"){
  read_csv(str_c(path, file)) %>% 
    pivot_longer(-country, names_to = "year", values_to = "value") %>% 
    mutate(variable = str_sub(file, 1, -5))
}
```
```{r, echo = TRUE}
read_gapminder("life_expectancy_years.csv")
```
här kan du ge kolumnen `variable` värdet `str_sub(file, 1, -5)`.

- Skapa en tabell (samma fyra kolumner) med alla variabler genom att använda `map_df` med argument `list.files("data/gapminder_raw/")` och `read_gapminder`.

***

## **v31**: APIer, databaser och webbskrap

### Mål denna vecka

- Använda grundläggande funktioner i paket som `httr`, `dbplyr` och `rvest`.

### Kursmaterial

#### Kommunicera med ett web-API

Ett vanligt sätt att kommunicera med en databas är via ett så kallat [REST API](https://sv.wikipedia.org/wiki/Representational_State_Transfer), där anrop sker genom en webbadress (url). För populära API
finns i allmänhet färdiga paket som förenklar kommunikationen, som t.ex. [rtweet](https://cran.r-project.org/web/packages/rtweet/), [spotifyr](https://cran.r-project.org/web/packages/spotifyr/index.html) och [rgbif](https://cran.r-project.org/web/packages/rgbif/index.html). Saknas sådant kan man använta paketet `httr` för att själv konstruera anrop, vi kommer använda funktionerna `GET` och `content` från detta paket.

##### A-$\pi$

[A-$\pi$](https://pi.delivery/#apipi_get) är ett enkelt API där du kan hämta en sekvens av $\pi$:s decimaler. Du behöver ange en startposition (`start`) och antal siffror (`numberOfDigits`), första 10 decimalerna ges av anropet [`http://api.pi.delivery/v1/pi?start=1&numberOfDigits=10`](http://api.pi.delivery/v1/pi?start=1&numberOfDigits=10) som vi gör från R med
```{r, echo = TRUE}
library(httr)
response <- GET("http://api.pi.delivery/v1/pi?start=1&numberOfDigits=10")
content(response)
```

##### Nobel API

På [https://nobelprize.readme.io/](https://nobelprize.readme.io/) finns ett API där du kan hämta data om nobelpriser och pristagare. Välj en kategori, t.ex. [prize](https://nobelprize.readme.io/docs/prize) och prova göra ett anrop under "Try it out" längst ned på sidan. Klicka "Try it!" och notera den URL som genereras i fönstret under. Väljer vi format `csv` och category `literature` blir denna `http://api.nobelprize.org/v1/prize.csv?category=literature`. För att hämta resultatet till R
```{r, echo = TRUE}
response <- GET("http://api.nobelprize.org/v1/prize.csv?category=literature")
content(response)
```


#### Skrapa webbsidor

Ibland saknas ett öppet API och vi blir tvungna att skrapa tabeller direkt från en webbsidas html-kod. Ett exempel gavs i föregående veckas uppgifter där vi hämtade tabeller över OS-medaljer från Wikipedia. För att lyckas med detta behövs grundläggande förståelse för 
hur html-koden är uppbyggd med [*taggar* och *element*](https://sv.wikipedia.org/wiki/HTML-element). Ett element består av en starttagg med eventuella attribut, innehåll och en sluttagg kodat som `<tagg attributnamn="värde">Innehåll</tagg>`. Informationen vi är ute efter är oftast `Innehåll` (men ibland `värde`).

Paketet `rvest` innehåller funktioner för att läsa webbsidor och extrahera information, vi illustrerar med ett minimalt exempel där vi vill få ut innehållet i taggen `b` ur `<a> 1 </a> <b> 2 </b>`. Först läser vi in html-koden
```{r, echo = TRUE}
page <- read_html("<a> 1 </a> <b> 2 </b>")
page
```
sedan extraherar vi taggen `b` med en så kallad [CSS-väljare](https://www.w3schools.com/cssref/css_selectors.asp) 
```{r, echo = TRUE}
elements <- html_elements(page, css = "b")
elements
```
slutligen drar vi ut innehållet som text
```{r, echo = TRUE}
html_text(elements)
```
Svårigheten ligger i allmänhet att hitta en CSS-väljare som plockar ut precis vad vi vill ha. I övningen med OS-medaljer vill vi 
extrahera en specifik tabell från en Wikipedia-sida. En tabell i standardformat skrivs i html med start- och sluttagg `<table> ... </table>`, vi kan därför prova med `css = "table"`
```{r, echo = TRUE}
url <- "https://sv.wikipedia.org/wiki/Medaljf%C3%B6rdelning_vid_olympiska_sommarspelen_2020"
page <- read_html(url)
elements <- html_elements(page, css = "table")
elements
```
Sidan innehåller fem tabeller där vi vill ha den första. Vi kan extrahera denna med
```{r, echo = TRUE}
html_table(elements[[1]])
```
men väljer istället att utnyttja `class`-attributet `wikitable` eftersom medaljtabellen inte ligger först på alla OS-sidor
```{r, echo = TRUE}
elements <- html_elements(page, css = ".wikitable")
html_table(elements[[1]])
```
Punkten i `.wikitable` anger att det just är `class`-attributet vi skall välja efter (se [CSS-väljare](https://www.w3schools.com/cssref/css_selectors.asp)). Ett verktyg som är användbart för att identifiera dessa är [SelectorGadget](https://selectorgadget.com/), som till exempel kan installeras som ett tillägg i Chrome. 


På linande sätt kan vi hämta aktuell tabell för Allsvenskan med
```{r, echo = TRUE}
url <- "https://www.allsvenskan.se/tabell"
page <- read_html(url)
elements <- html_elements(page, css = ".col-md-6")
html_table(elements[[1]])
```
där SelectorGadget har använts för att hitta väljaren `.col-md-6`. Notera att rubrikerna behöver justering; tabellen är ju inte avsedd för skrapning och resultatet behöver därför ofta justeras. Vid webbskrapning är det viktigt att respektera att syftet i allmänhet inte är att dela data och därför inte belasta servrar i onödan. Upphovsrätten till data kan även vara skyddat genom [katalogskyddet](https://lagen.nu/1960:729#P49).

### Övningar

##### Nobel API

- Hämta alla pristagare ([Laureate](https://nobelprize.readme.io/docs/laureate)) och bestäm andelen män i varje priskategori.

##### SR API

Sveriges radio har ett [öppet API](https://sverigesradio.se/oppetapi) där man bland annat kan hämta [låtlistor från en given kanal och datum](https://sverigesradio.se/api/documentation/v2/metoder/musik.html). Svaret ges dock inte i vanligt csv-format utan som [XML](https://sv.wikipedia.org/wiki/XML) eller [JSON](https://sv.wikipedia.org/wiki/JSON), vilket är vanligt då dessa format är mer flexibla. Ofta är JSON enklare att arbeta med i R.

- Konstruera en funktion
```{r, echo = TRUE, eval = FALSE}
get_songs <- function(channel, date){
  ...
}
```
som hämtar låtlistan för ett givet [kanalnummer](http://api.sr.se/api/v2/channels/) och datum. Begär svaret i JSON genom att avsluta anropet med `&format=json`. Du kan konvertera det svar (`response`) du får av `GET` med `content(response, "text") %>% jsonlite::fromJSON()`.

##### Skrapa Dramaten

På [Dramatens arkiv Rollboken](https://old.dramaten.se/medverkande/rollboken) kan man söka efter uppgifter om deras produktioner. Eftersom de tabeller som genereras inte är i standardformat blir det enklast att skrapa varje variabel för sig. Namn på [uppsättningarna från 2019](https://old.dramaten.se/medverkande/rollboken/?category=date&query=2019) får vi t.ex. med
```{r, echo = TRUE}
url <- "https://old.dramaten.se/medverkande/rollboken/?category=date&query=2019"
page <- read_html(url)
elements <- html_elements(page, css = ".play-name")
html_text(elements)[-1]
```
(här tar `[-1]` bort första elementet, rubriken).

- Identifiera CSS-väljare och skrapa övriga kolumner i tabellen.


***

## **v32**: Mer om visualisering

